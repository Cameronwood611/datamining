{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af53331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import scale,RobustScaler,StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6f50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec9350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "titles = list(raw['title'])\n",
    "le.fit(titles)\n",
    "encoded_titles = le.transform(titles)\n",
    "raw['title'] = encoded_titles\n",
    "\n",
    "columns = ['title','runtime', 'imdb_score', 'imdb_votes', 'user_review_count', 'critic_review_count','critic_overall_score','critic_positive_score','critic_mixed_score','critic_negative_score','user_overall_score','user_positive_score','user_mixed_score','user_negative_score','#_of_nominations','#_of_wins']\n",
    "\n",
    "for col in columns:\n",
    "    raw[col] = raw[col].replace('[^.0-9]', '', regex=True,).astype(float).fillna(0.0)\n",
    "\n",
    "for col in raw.select_dtypes(include='object').columns:\n",
    "        raw[col] = raw[col].str.upper().replace('Z_','',regex=True).replace('[^A-Z]','',regex=True)\n",
    "\n",
    "data_types = {f:t for f,t in zip(raw.columns,raw.dtypes)}\n",
    "\n",
    "# copy df\n",
    "df = raw.copy()\n",
    "df = df.drop(labels=['title'],axis=1)\n",
    "df = df.drop(labels=['#_of_wins'],axis=1)\n",
    "\n",
    "data_meta = pd.DataFrame(df.nunique(),columns=['num'],index=None).sort_values('num').reset_index()\n",
    "data_meta.columns = ['name','num']\n",
    "data_meta['type'] = 'numerical'\n",
    "\n",
    "#data_meta.loc[(data_meta['num']<=15) & (~data_meta['name'].isin(['category','name','winner'])),'type']='categorical'\n",
    "data_meta.loc[data_meta['name'].isin(['category','name']),'type']='categorical'\n",
    "\n",
    "categorical_features = list(data_meta.loc[data_meta['type']=='categorical','name'])\n",
    "numerical_features = list(data_meta.loc[data_meta['type']=='numerical','name'])\n",
    "\n",
    "\n",
    "# Standard Scaled \"mean normalisation\"\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[numerical_features])\n",
    "#print(\"Feature Means:\\t\",[f\"{x:.2}\" for x in scaler.mean_])\n",
    "numerical_data = scaler.transform(df[numerical_features])\n",
    "numerical_data = pd.DataFrame(numerical_data,index=df.index,columns=numerical_features)\n",
    "numerical_data[:2]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(df[numerical_features])\n",
    "numerical_data = scaler.transform(df[numerical_features])\n",
    "numerical_data = pd.DataFrame(numerical_data,index=df.index,columns=numerical_features)\n",
    "numerical_data[:2]\n",
    "\n",
    "# transformed and scaled dataset\n",
    "Xy_scaled = pd.concat([numerical_data],axis=1)\n",
    "#print(f'Data min:max {Xy_scaled.min().min(),Xy_scaled.max().max()}')\n",
    "\n",
    "# original data\n",
    "Xy_original = df[numerical_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dbf6297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.44986421 0.19022574]\n",
      "Cumulative variance explained by 2 principal components: 64.01%\n"
     ]
    }
   ],
   "source": [
    "#reducing dimension using PCA.\n",
    "#calculates the % of the data we still have of our original\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2result=pca_2.fit_transform(Xy_scaled)\n",
    "print('Explained variation per principal component: {}'.format(pca_2.explained_variance_ratio_))\n",
    "print('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca_2.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84bfbc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.read_csv('test_data.csv')\n",
    "\n",
    "titles_pred = list(pred['title'])\n",
    "le.fit(titles_pred)\n",
    "titles_pred = le.transform(titles_pred)\n",
    "pred['title'] = titles_pred\n",
    "\n",
    "\n",
    "columns_pred = ['title','runtime', 'imdb_score', 'imdb_votes', 'user_review_count', 'critic_review_count','critic_overall_score','critic_positive_score','critic_mixed_score','critic_negative_score','user_overall_score','user_positive_score','user_mixed_score','user_negative_score','#_of_nominations','#_of_wins']\n",
    "\n",
    "\n",
    "    \n",
    "for col in columns_pred:\n",
    "    pred[col] = pred[col].replace('[^.0-9]', '', regex=True,).astype(float).fillna(0.0)\n",
    "\n",
    "    \n",
    "\n",
    "for col in pred.select_dtypes(include='object').columns:\n",
    "        pred[col] = pred[col].str.upper().replace('Z_','',regex=True).replace('[^A-Z]','',regex=True)\n",
    "\n",
    "dp = pred.copy()\n",
    "dp = dp.drop(labels=['title'],axis=1)\n",
    "dp = dp.drop(labels=['#_of_wins'],axis=1)\n",
    "\n",
    "#print(data_count)\n",
    "data_count_dp = pd.DataFrame(dp.nunique(),columns=['num'],index=None).sort_values('num').reset_index()\n",
    "data_count_dp.columns = ['name','num']\n",
    "data_count_dp['type'] = 'numerical'\n",
    "\n",
    "#data_count_dp.loc[data_count_dp['name'].isin(['category']),'type']='nominal'\n",
    "\n",
    "nominal_features_dp = list(data_count_dp.loc[data_count_dp['type']=='nominal','name'])\n",
    "numerical_features_dp = list(data_count_dp.loc[data_count_dp['type']=='numerical','name'])\n",
    "\n",
    "scaler_dp = StandardScaler()\n",
    "scaler_dp.fit(dp[numerical_features_dp])\n",
    "#print(\"Feature Means:\\t\",[f\"{x:.2}\" for x in scaler_dp.mean_])\n",
    "numerical_data_dp = scaler_dp.transform(dp[numerical_features_dp])\n",
    "numerical_data_dp = pd.DataFrame(numerical_data_dp,index=dp.index,columns=numerical_features_dp)\n",
    "numerical_data_dp[:2]\n",
    "\n",
    "scaler_dp = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_dp.fit(dp[numerical_features_dp])\n",
    "numerical_data_dp = scaler_dp.transform(dp[numerical_features_dp])\n",
    "numerical_data_dp = pd.DataFrame(numerical_data_dp,index=dp.index,columns=numerical_features_dp)\n",
    "\n",
    "\n",
    "# transformed and scaled dataset\n",
    "Xy_scaled_dp = pd.concat([numerical_data_dp],axis=1)\n",
    "#print(f'Data min:max {Xy_scaled.min().min(),Xy_scaled.max().max()}')\n",
    "\n",
    "#print(Xy_scaled_dp.shape)\n",
    "# original data\n",
    "Xy_original_dp = dp[numerical_features_dp].copy()\n",
    "\n",
    "column_names_dp = Xy_scaled_dp.columns.values\n",
    "column_names_dp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54a47fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.48296128 0.26214879]\n",
      "Cumulative variance explained by 2 principal components: 74.51%\n"
     ]
    }
   ],
   "source": [
    "#reducing dimension using PCA.\n",
    "#calculates the % of the data we still have of our original\n",
    "pca_3 = PCA(n_components=2)\n",
    "pca_3result=pca_2.fit_transform(Xy_scaled_dp)\n",
    "print('Explained variation per principal component: {}'.format(pca_2.explained_variance_ratio_))\n",
    "print('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca_2.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bface609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = raw.copy()\n",
    "train = train['#_of_wins']\n",
    "\n",
    "train2 = pd.read_csv('test_data.csv')\n",
    "train2 = train2['#_of_wins']\n",
    "\n",
    "X_train = pca_2result\n",
    "y_train = train\n",
    "\n",
    "X_test = pca_3result\n",
    "y_test = train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19a97c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 12 points : 7\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abca55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
